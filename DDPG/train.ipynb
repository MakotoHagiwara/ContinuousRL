{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 Data collected\n",
      "Episode 0 finished | Episode reward -1534.367660\n",
      "Episode 0 finished | Average reward -1246.041005\n",
      "loss/critic 0.9218184947967529\n",
      "loss/actor 12.491300582885742\n",
      "loss/critic 0.6930734515190125\n",
      "loss/actor 15.819133758544922\n",
      "loss/critic 0.6767975687980652\n",
      "loss/actor 20.09617042541504\n",
      "loss/critic 1.2657431364059448\n",
      "loss/actor 27.29192352294922\n",
      "Episode 20 finished | Episode reward -1132.803531\n",
      "Episode 20 finished | Average reward -1440.696501\n",
      "loss/critic 0.4763389825820923\n",
      "loss/actor 30.72571563720703\n",
      "loss/critic 1.1937825679779053\n",
      "loss/actor 38.67076873779297\n",
      "loss/critic 0.8999608755111694\n",
      "loss/actor 41.87420654296875\n",
      "loss/critic 1.2929565906524658\n",
      "loss/actor 46.78300476074219\n",
      "Episode 40 finished | Episode reward -1466.030796\n",
      "Episode 40 finished | Average reward -1412.640073\n",
      "loss/critic 0.6344180703163147\n",
      "loss/actor 52.198089599609375\n",
      "loss/critic 1.0720306634902954\n",
      "loss/actor 59.16401290893555\n",
      "loss/critic 0.6462262868881226\n",
      "loss/actor 59.24747848510742\n",
      "loss/critic 2.165419101715088\n",
      "loss/actor 64.20437622070312\n",
      "Episode 60 finished | Episode reward -1009.106093\n",
      "Episode 60 finished | Average reward -1279.679167\n",
      "loss/critic 1.7506523132324219\n",
      "loss/actor 72.13814544677734\n",
      "loss/critic 1.2546221017837524\n",
      "loss/actor 75.21626281738281\n",
      "loss/critic 2.1787707805633545\n",
      "loss/actor 75.6251449584961\n",
      "loss/critic 2.2398695945739746\n",
      "loss/actor 75.61760711669922\n",
      "Episode 80 finished | Episode reward -395.267702\n",
      "Episode 80 finished | Average reward -712.949448\n",
      "loss/critic 1.7705259323120117\n",
      "loss/actor 73.91130828857422\n",
      "loss/critic 2.4566802978515625\n",
      "loss/actor 74.01025390625\n",
      "loss/critic 1.654002070426941\n",
      "loss/actor 78.06594848632812\n",
      "loss/critic 1.3807140588760376\n",
      "loss/actor 78.50154113769531\n",
      "Episode 100 finished | Episode reward -130.004209\n",
      "Episode 100 finished | Average reward -510.337521\n",
      "loss/critic 4.3531599044799805\n",
      "loss/actor 79.02590942382812\n",
      "loss/critic 2.6198983192443848\n",
      "loss/actor 78.76126861572266\n",
      "loss/critic 1.898196816444397\n",
      "loss/actor 79.14378356933594\n",
      "loss/critic 1.3837430477142334\n",
      "loss/actor 78.82656860351562\n",
      "Episode 120 finished | Episode reward -123.019011\n",
      "Episode 120 finished | Average reward -154.026985\n",
      "loss/critic 1.3278636932373047\n",
      "loss/actor 80.43327331542969\n",
      "loss/critic 1.338474988937378\n",
      "loss/actor 78.33740997314453\n",
      "loss/critic 2.3750884532928467\n",
      "loss/actor 73.34405517578125\n",
      "loss/critic 1.4424641132354736\n",
      "loss/actor 75.6523208618164\n",
      "Episode 140 finished | Episode reward -128.865527\n",
      "Episode 140 finished | Average reward -141.987802\n",
      "loss/critic 1.3107471466064453\n",
      "loss/actor 67.72754669189453\n",
      "loss/critic 1.4475178718566895\n",
      "loss/actor 77.38381958007812\n",
      "loss/critic 1.2274715900421143\n",
      "loss/actor 70.29071044921875\n",
      "loss/critic 2.3942532539367676\n",
      "loss/actor 74.5613021850586\n",
      "Episode 160 finished | Episode reward -124.896041\n",
      "Episode 160 finished | Average reward -160.760283\n",
      "loss/critic 1.7238963842391968\n",
      "loss/actor 71.98350524902344\n",
      "loss/critic 1.1833231449127197\n",
      "loss/actor 77.23259735107422\n",
      "loss/critic 1.5956058502197266\n",
      "loss/actor 70.98324584960938\n",
      "loss/critic 2.06022310256958\n",
      "loss/actor 68.41400909423828\n",
      "Episode 180 finished | Episode reward -124.202285\n",
      "Episode 180 finished | Average reward -135.995054\n",
      "loss/critic 2.309408664703369\n",
      "loss/actor 71.57853698730469\n",
      "loss/critic 1.1325510740280151\n",
      "loss/actor 71.63410949707031\n",
      "loss/critic 1.1759999990463257\n",
      "loss/actor 59.729454040527344\n",
      "loss/critic 1.1380151510238647\n",
      "loss/actor 62.1883544921875\n",
      "Episode 200 finished | Episode reward -236.856929\n",
      "Episode 200 finished | Average reward -136.692735\n",
      "loss/critic 1.020753264427185\n",
      "loss/actor 65.56742095947266\n",
      "loss/critic 1.2429147958755493\n",
      "loss/actor 62.67674255371094\n",
      "loss/critic 2.4348087310791016\n",
      "loss/actor 54.275390625\n",
      "loss/critic 1.3475888967514038\n",
      "loss/actor 56.22821044921875\n",
      "Episode 220 finished | Episode reward -2.273754\n",
      "Episode 220 finished | Average reward -131.450547\n",
      "loss/critic 1.633307695388794\n",
      "loss/actor 54.27265930175781\n",
      "loss/critic 0.8337017893791199\n",
      "loss/actor 52.74951934814453\n",
      "loss/critic 1.1146334409713745\n",
      "loss/actor 65.14848327636719\n",
      "loss/critic 1.775466799736023\n",
      "loss/actor 51.784393310546875\n",
      "Episode 240 finished | Episode reward -248.257203\n",
      "Episode 240 finished | Average reward -159.336252\n",
      "loss/critic 1.857327938079834\n",
      "loss/actor 54.69602966308594\n",
      "loss/critic 2.259305953979492\n",
      "loss/actor 43.45150375366211\n",
      "loss/critic 0.7020608186721802\n",
      "loss/actor 44.78593826293945\n",
      "loss/critic 0.89189612865448\n",
      "loss/actor 51.156734466552734\n",
      "Episode 260 finished | Episode reward -123.428607\n",
      "Episode 260 finished | Average reward -148.726041\n",
      "loss/critic 1.0979663133621216\n",
      "loss/actor 52.8072509765625\n",
      "loss/critic 2.9216818809509277\n",
      "loss/actor 52.51841735839844\n",
      "loss/critic 1.2063124179840088\n",
      "loss/actor 47.923828125\n",
      "loss/critic 0.9531633257865906\n",
      "loss/actor 52.81345748901367\n",
      "Episode 280 finished | Episode reward -234.398120\n",
      "Episode 280 finished | Average reward -164.738956\n",
      "loss/critic 1.0674386024475098\n",
      "loss/actor 48.04427719116211\n",
      "loss/critic 2.0444698333740234\n",
      "loss/actor 50.272239685058594\n",
      "loss/critic 1.0245180130004883\n",
      "loss/actor 42.820369720458984\n",
      "loss/critic 0.9816067814826965\n",
      "loss/actor 46.32027816772461\n",
      "Episode 300 finished | Episode reward -121.703692\n",
      "Episode 300 finished | Average reward -150.798822\n",
      "loss/critic 1.8510091304779053\n",
      "loss/actor 44.14580535888672\n",
      "loss/critic 0.8033674955368042\n",
      "loss/actor 32.51618576049805\n",
      "loss/critic 1.618870735168457\n",
      "loss/actor 41.99906539916992\n",
      "loss/critic 0.6638317108154297\n",
      "loss/actor 35.90323257446289\n",
      "Episode 320 finished | Episode reward -125.611105\n",
      "Episode 320 finished | Average reward -138.698433\n",
      "loss/critic 0.970617949962616\n",
      "loss/actor 40.98419189453125\n",
      "loss/critic 3.060711622238159\n",
      "loss/actor 47.45950698852539\n",
      "loss/critic 0.9140622615814209\n",
      "loss/actor 42.13228988647461\n",
      "loss/critic 1.0794854164123535\n",
      "loss/actor 33.019187927246094\n",
      "Episode 340 finished | Episode reward -113.556936\n",
      "Episode 340 finished | Average reward -156.271410\n",
      "loss/critic 1.1688461303710938\n",
      "loss/actor 47.75484848022461\n",
      "loss/critic 2.7581164836883545\n",
      "loss/actor 37.709617614746094\n",
      "loss/critic 0.818405270576477\n",
      "loss/actor 41.314815521240234\n",
      "loss/critic 0.9390813112258911\n",
      "loss/actor 32.025665283203125\n",
      "Episode 360 finished | Episode reward -123.345822\n",
      "Episode 360 finished | Average reward -152.800333\n",
      "loss/critic 0.882027268409729\n",
      "loss/actor 32.17790603637695\n",
      "loss/critic 1.1335326433181763\n",
      "loss/actor 36.21568298339844\n",
      "loss/critic 1.5484232902526855\n",
      "loss/actor 36.171173095703125\n",
      "loss/critic 0.6430081725120544\n",
      "loss/actor 45.27544021606445\n",
      "Episode 380 finished | Episode reward -233.643127\n",
      "Episode 380 finished | Average reward -157.226046\n",
      "loss/critic 0.8199546933174133\n",
      "loss/actor 40.54228210449219\n",
      "loss/critic 1.0053620338439941\n",
      "loss/actor 36.102317810058594\n",
      "loss/critic 2.089428663253784\n",
      "loss/actor 39.85838317871094\n",
      "loss/critic 0.7862176895141602\n",
      "loss/actor 33.38376235961914\n",
      "Episode 400 finished | Episode reward -350.284512\n",
      "Episode 400 finished | Average reward -181.422619\n",
      "loss/critic 1.9876856803894043\n",
      "loss/actor 39.5434684753418\n",
      "loss/critic 0.9080606698989868\n",
      "loss/actor 30.474273681640625\n",
      "loss/critic 0.6259264945983887\n",
      "loss/actor 24.687652587890625\n",
      "loss/critic 1.0717198848724365\n",
      "loss/actor 33.72706604003906\n",
      "Episode 420 finished | Episode reward -124.187042\n",
      "Episode 420 finished | Average reward -177.310411\n",
      "loss/critic 0.9505434632301331\n",
      "loss/actor 38.2977409362793\n",
      "loss/critic 0.8123419284820557\n",
      "loss/actor 27.4871826171875\n",
      "loss/critic 0.8544972538948059\n",
      "loss/actor 24.322797775268555\n",
      "loss/critic 0.9599844217300415\n",
      "loss/actor 31.179309844970703\n",
      "Episode 440 finished | Episode reward -115.439350\n",
      "Episode 440 finished | Average reward -149.988625\n",
      "loss/critic 0.9879405498504639\n",
      "loss/actor 30.63461685180664\n",
      "loss/critic 0.7576091289520264\n",
      "loss/actor 28.353721618652344\n",
      "loss/critic 0.8176167011260986\n",
      "loss/actor 34.634910583496094\n",
      "loss/critic 0.6434570550918579\n",
      "loss/actor 25.54047203063965\n",
      "Episode 460 finished | Episode reward -122.311661\n",
      "Episode 460 finished | Average reward -159.934435\n",
      "loss/critic 0.6752915382385254\n",
      "loss/actor 28.261966705322266\n",
      "loss/critic 0.6218785643577576\n",
      "loss/actor 25.322490692138672\n",
      "loss/critic 0.7059213519096375\n",
      "loss/actor 20.603076934814453\n",
      "loss/critic 0.7880416512489319\n",
      "loss/actor 21.1296443939209\n",
      "Episode 480 finished | Episode reward -127.490514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 480 finished | Average reward -167.045454\n",
      "loss/critic 1.8633116483688354\n",
      "loss/actor 25.75960922241211\n",
      "loss/critic 0.3595045208930969\n",
      "loss/actor 18.07027816772461\n",
      "loss/critic 0.42204660177230835\n",
      "loss/actor 17.088184356689453\n",
      "loss/critic 0.4920065402984619\n",
      "loss/actor 19.111093521118164\n",
      "Episode 500 finished | Episode reward -122.382928\n",
      "Episode 500 finished | Average reward -157.760209\n",
      "loss/critic 0.8735018968582153\n",
      "loss/actor 11.40994930267334\n",
      "loss/critic 0.666684627532959\n",
      "loss/actor 19.620346069335938\n",
      "loss/critic 0.6897068619728088\n",
      "loss/actor 15.01485824584961\n",
      "loss/critic 0.5286005139350891\n",
      "loss/actor 8.325299263000488\n",
      "Episode 520 finished | Episode reward -120.565080\n",
      "Episode 520 finished | Average reward -150.653989\n",
      "loss/critic 0.4266456663608551\n",
      "loss/actor 18.75041961669922\n",
      "loss/critic 0.524056077003479\n",
      "loss/actor 11.819513320922852\n",
      "loss/critic 0.6108249425888062\n",
      "loss/actor 9.664100646972656\n",
      "loss/critic 0.32520121335983276\n",
      "loss/actor 5.1766767501831055\n",
      "Episode 540 finished | Episode reward -1.131196\n",
      "Episode 540 finished | Average reward -155.890309\n",
      "loss/critic 0.29375165700912476\n",
      "loss/actor 4.024622440338135\n",
      "loss/critic 0.5462429523468018\n",
      "loss/actor 9.847558975219727\n",
      "loss/critic 0.36922159790992737\n",
      "loss/actor 3.6199145317077637\n",
      "loss/critic 0.3394162058830261\n",
      "loss/actor 4.028803825378418\n",
      "Episode 560 finished | Episode reward -118.582178\n",
      "Episode 560 finished | Average reward -312.137680\n",
      "loss/critic 0.26446399092674255\n",
      "loss/actor 2.502413272857666\n",
      "loss/critic 0.3614177107810974\n",
      "loss/actor 4.356968879699707\n",
      "loss/critic 0.4932883381843567\n",
      "loss/actor 3.8368325233459473\n",
      "loss/critic 0.6262893080711365\n",
      "loss/actor 3.5660929679870605\n",
      "Episode 580 finished | Episode reward -1.406888\n",
      "Episode 580 finished | Average reward -676.511318\n",
      "loss/critic 0.5302848815917969\n",
      "loss/actor 3.364877700805664\n",
      "loss/critic 0.5245471596717834\n",
      "loss/actor 1.902249813079834\n",
      "loss/critic 0.39770013093948364\n",
      "loss/actor 3.8947763442993164\n",
      "loss/critic 0.4850965142250061\n",
      "loss/actor 4.803399085998535\n",
      "Episode 600 finished | Episode reward -223.290111\n",
      "Episode 600 finished | Average reward -631.853515\n",
      "loss/critic 0.5889461636543274\n",
      "loss/actor -2.5605664253234863\n",
      "loss/critic 0.31594038009643555\n",
      "loss/actor -0.5551974177360535\n",
      "loss/critic 0.31746232509613037\n",
      "loss/actor -1.6019947528839111\n",
      "loss/critic 1.0500822067260742\n",
      "loss/actor 6.645251274108887\n",
      "Episode 620 finished | Episode reward -1238.158295\n",
      "Episode 620 finished | Average reward -989.898552\n",
      "loss/critic 0.4225722849369049\n",
      "loss/actor -1.57024347782135\n",
      "loss/critic 0.6616066694259644\n",
      "loss/actor -1.7173153162002563\n",
      "loss/critic 0.3705979883670807\n",
      "loss/actor 1.5452375411987305\n",
      "loss/critic 0.41659972071647644\n",
      "loss/actor 2.2367424964904785\n",
      "Episode 640 finished | Episode reward -239.844458\n",
      "Episode 640 finished | Average reward -175.848020\n",
      "loss/critic 0.5470648407936096\n",
      "loss/actor 1.561161756515503\n",
      "loss/critic 0.6376925706863403\n",
      "loss/actor 3.4812397956848145\n",
      "loss/critic 0.3807251453399658\n",
      "loss/actor -1.388614535331726\n",
      "loss/critic 0.4651646316051483\n",
      "loss/actor 0.4789847433567047\n",
      "Episode 660 finished | Episode reward -345.813882\n",
      "Episode 660 finished | Average reward -255.053375\n",
      "loss/critic 0.3792847990989685\n",
      "loss/actor -1.125229001045227\n",
      "loss/critic 0.4712333381175995\n",
      "loss/actor -0.2532113194465637\n",
      "loss/critic 0.4708912968635559\n",
      "loss/actor 1.749179482460022\n",
      "loss/critic 0.7878435850143433\n",
      "loss/actor 7.846538543701172\n",
      "Episode 680 finished | Episode reward -351.326342\n",
      "Episode 680 finished | Average reward -174.521841\n",
      "loss/critic 0.5687134265899658\n",
      "loss/actor 2.130664587020874\n",
      "loss/critic 0.45462507009506226\n",
      "loss/actor -1.5827850103378296\n",
      "loss/critic 0.40780141949653625\n",
      "loss/actor 1.3735026121139526\n",
      "loss/critic 0.45001548528671265\n",
      "loss/actor 2.0082314014434814\n",
      "Episode 700 finished | Episode reward -0.927245\n",
      "Episode 700 finished | Average reward -153.530031\n",
      "loss/critic 0.357136994600296\n",
      "loss/actor -2.703909158706665\n",
      "loss/critic 0.5083502531051636\n",
      "loss/actor -1.7841205596923828\n",
      "loss/critic 0.2844075858592987\n",
      "loss/actor -3.3521318435668945\n",
      "loss/critic 0.3031400144100189\n",
      "loss/actor -1.9364900588989258\n",
      "Episode 720 finished | Episode reward -119.564739\n",
      "Episode 720 finished | Average reward -150.567685\n",
      "loss/critic 0.25572681427001953\n",
      "loss/actor -0.9207693338394165\n",
      "loss/critic 0.2621300220489502\n",
      "loss/actor -2.5751113891601562\n",
      "loss/critic 0.2287030667066574\n",
      "loss/actor -4.05803108215332\n",
      "loss/critic 0.34089580178260803\n",
      "loss/actor -1.4650890827178955\n",
      "Episode 740 finished | Episode reward -129.355083\n",
      "Episode 740 finished | Average reward -146.370237\n",
      "loss/critic 0.3608456552028656\n",
      "loss/actor -0.8757436275482178\n",
      "loss/critic 0.5072966814041138\n",
      "loss/actor -4.90514612197876\n",
      "loss/critic 0.4124646484851837\n",
      "loss/actor -1.3611421585083008\n",
      "loss/critic 0.6516711115837097\n",
      "loss/actor -3.137331008911133\n",
      "Episode 760 finished | Episode reward -338.954820\n",
      "Episode 760 finished | Average reward -151.994736\n",
      "loss/critic 0.25887811183929443\n",
      "loss/actor 0.03962206840515137\n",
      "loss/critic 0.312165230512619\n",
      "loss/actor 0.5219515562057495\n",
      "loss/critic 0.28755566477775574\n",
      "loss/actor -0.5640353560447693\n",
      "loss/critic 0.58599853515625\n",
      "loss/actor 1.0931763648986816\n",
      "Episode 780 finished | Episode reward -129.435579\n",
      "Episode 780 finished | Average reward -213.273860\n",
      "loss/critic 0.2827465236186981\n",
      "loss/actor 0.22979965806007385\n",
      "loss/critic 0.8236467838287354\n",
      "loss/actor 3.988332748413086\n",
      "loss/critic 0.2840329706668854\n",
      "loss/actor -3.1825246810913086\n",
      "loss/critic 0.6218725442886353\n",
      "loss/actor 0.49293971061706543\n",
      "Episode 800 finished | Episode reward -383.243011\n",
      "Episode 800 finished | Average reward -189.693694\n",
      "loss/critic 0.26478666067123413\n",
      "loss/actor -2.6646294593811035\n",
      "loss/critic 0.24471300840377808\n",
      "loss/actor -4.807804107666016\n",
      "loss/critic 0.45214518904685974\n",
      "loss/actor 1.3326060771942139\n",
      "loss/critic 0.2979889512062073\n",
      "loss/actor -3.2297348976135254\n",
      "Episode 820 finished | Episode reward -285.339880\n",
      "Episode 820 finished | Average reward -221.353757\n",
      "loss/critic 0.3246806859970093\n",
      "loss/actor -2.299111843109131\n",
      "loss/critic 0.3488004505634308\n",
      "loss/actor -1.6555266380310059\n",
      "loss/critic 0.3241901099681854\n",
      "loss/actor -1.972336769104004\n",
      "loss/critic 0.416523814201355\n",
      "loss/actor 1.308853268623352\n",
      "Episode 840 finished | Episode reward -137.138687\n",
      "Episode 840 finished | Average reward -198.322018\n",
      "loss/critic 0.43936586380004883\n",
      "loss/actor -0.8638916611671448\n",
      "loss/critic 0.4724293351173401\n",
      "loss/actor 1.3519539833068848\n",
      "loss/critic 0.3649468421936035\n",
      "loss/actor -2.7832131385803223\n",
      "loss/critic 0.4165019690990448\n",
      "loss/actor -1.225335717201233\n",
      "Episode 860 finished | Episode reward -507.701822\n",
      "Episode 860 finished | Average reward -261.684756\n",
      "loss/critic 0.5163908004760742\n",
      "loss/actor 0.7950589060783386\n",
      "loss/critic 0.3466232120990753\n",
      "loss/actor -3.3420820236206055\n",
      "loss/critic 0.5352626442909241\n",
      "loss/actor -2.236445426940918\n",
      "loss/critic 0.5750859975814819\n",
      "loss/actor 0.8081340789794922\n",
      "Episode 880 finished | Episode reward -413.628409\n",
      "Episode 880 finished | Average reward -152.309401\n",
      "loss/critic 0.8023467063903809\n",
      "loss/actor 2.7441000938415527\n",
      "loss/critic 0.24321913719177246\n",
      "loss/actor -1.3789162635803223\n",
      "loss/critic 0.38431215286254883\n",
      "loss/actor -0.08321011066436768\n",
      "loss/critic 0.31678715348243713\n",
      "loss/actor -1.5774171352386475\n",
      "Episode 900 finished | Episode reward -4.623068\n",
      "Episode 900 finished | Average reward -163.876340\n",
      "loss/critic 0.6705935001373291\n",
      "loss/actor 2.468001365661621\n",
      "loss/critic 0.35854223370552063\n",
      "loss/actor -2.3733363151550293\n",
      "loss/critic 0.3802715241909027\n",
      "loss/actor 1.5311594009399414\n",
      "loss/critic 0.2778644561767578\n",
      "loss/actor 0.3787343204021454\n",
      "Episode 920 finished | Episode reward -135.741566\n",
      "Episode 920 finished | Average reward -199.792603\n",
      "loss/critic 0.2494240701198578\n",
      "loss/actor -2.05332612991333\n",
      "loss/critic 0.4879997670650482\n",
      "loss/actor 3.041090488433838\n",
      "loss/critic 0.7144039869308472\n",
      "loss/actor 0.674774169921875\n",
      "loss/critic 0.365993857383728\n",
      "loss/actor 2.3545639514923096\n",
      "Episode 940 finished | Episode reward -128.144296\n",
      "Episode 940 finished | Average reward -230.399398\n",
      "loss/critic 0.43337953090667725\n",
      "loss/actor 1.9861516952514648\n",
      "loss/critic 0.48139381408691406\n",
      "loss/actor 2.5282986164093018\n",
      "loss/critic 0.42157408595085144\n",
      "loss/actor 4.498391628265381\n",
      "loss/critic 0.520879328250885\n",
      "loss/actor 3.076079845428467\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 960 finished | Episode reward -128.437784\n",
      "Episode 960 finished | Average reward -199.721197\n",
      "loss/critic 0.30768483877182007\n",
      "loss/actor 1.8846532106399536\n",
      "loss/critic 0.5321428179740906\n",
      "loss/actor 6.563823699951172\n",
      "loss/critic 0.32113543152809143\n",
      "loss/actor 3.785130500793457\n",
      "loss/critic 0.4256055951118469\n",
      "loss/actor 3.290717840194702\n",
      "Episode 980 finished | Episode reward -372.796312\n",
      "Episode 980 finished | Average reward -132.055528\n",
      "loss/critic 0.3380940854549408\n",
      "loss/actor 3.3096935749053955\n",
      "loss/critic 0.4138662815093994\n",
      "loss/actor 2.5006422996520996\n",
      "loss/critic 0.43100666999816895\n",
      "loss/actor 1.2917671203613281\n",
      "loss/critic 0.44916436076164246\n",
      "loss/actor 3.5561985969543457\n",
      "Episode 1000 finished | Episode reward -124.100458\n",
      "Episode 1000 finished | Average reward -183.656364\n",
      "loss/critic 0.4459836184978485\n",
      "loss/actor 5.723635196685791\n",
      "loss/critic 0.459855318069458\n",
      "loss/actor 3.2910847663879395\n",
      "loss/critic 0.35270410776138306\n",
      "loss/actor 3.4542236328125\n",
      "loss/critic 0.3075803518295288\n",
      "loss/actor 0.45879173278808594\n",
      "Episode 1020 finished | Episode reward -496.307290\n",
      "Episode 1020 finished | Average reward -206.398869\n",
      "loss/critic 0.6262677907943726\n",
      "loss/actor 4.082127094268799\n",
      "loss/critic 0.29903221130371094\n",
      "loss/actor 1.545121431350708\n",
      "loss/critic 0.29721343517303467\n",
      "loss/actor 1.4884529113769531\n",
      "loss/critic 0.40750372409820557\n",
      "loss/actor 4.8510003089904785\n",
      "Episode 1040 finished | Episode reward -134.995607\n",
      "Episode 1040 finished | Average reward -151.785835\n",
      "loss/critic 0.394290953874588\n",
      "loss/actor 2.965984582901001\n",
      "loss/critic 0.29930177330970764\n",
      "loss/actor 2.4836983680725098\n",
      "loss/critic 0.4824342727661133\n",
      "loss/actor 4.2631330490112305\n",
      "loss/critic 0.44577014446258545\n",
      "loss/actor 4.564970016479492\n",
      "Episode 1060 finished | Episode reward -245.401309\n",
      "Episode 1060 finished | Average reward -151.128122\n",
      "loss/critic 0.26841649413108826\n",
      "loss/actor 0.9305800199508667\n",
      "loss/critic 0.4693621098995209\n",
      "loss/actor 3.865809917449951\n",
      "loss/critic 0.43230387568473816\n",
      "loss/actor 0.003071427345275879\n",
      "loss/critic 0.3481236696243286\n",
      "loss/actor 0.1588432788848877\n",
      "Episode 1080 finished | Episode reward -240.741188\n",
      "Episode 1080 finished | Average reward -151.380162\n",
      "loss/critic 0.26346471905708313\n",
      "loss/actor -0.579509973526001\n",
      "loss/critic 0.40690070390701294\n",
      "loss/actor 4.203473091125488\n",
      "loss/critic 0.18023419380187988\n",
      "loss/actor -0.4696093201637268\n",
      "loss/critic 0.26136791706085205\n",
      "loss/actor 2.2796318531036377\n",
      "Episode 1100 finished | Episode reward -129.055276\n",
      "Episode 1100 finished | Average reward -173.219491\n",
      "loss/critic 0.24406088888645172\n",
      "loss/actor 3.6804087162017822\n",
      "loss/critic 0.31369903683662415\n",
      "loss/actor 1.5409677028656006\n",
      "loss/critic 0.3607511520385742\n",
      "loss/actor 1.0092856884002686\n",
      "loss/critic 0.32164230942726135\n",
      "loss/actor 0.6905553936958313\n",
      "Episode 1120 finished | Episode reward -120.425768\n",
      "Episode 1120 finished | Average reward -168.224314\n",
      "loss/critic 0.7187378406524658\n",
      "loss/actor -0.43752485513687134\n",
      "loss/critic 0.21742504835128784\n",
      "loss/actor 1.2885781526565552\n",
      "loss/critic 0.2578960061073303\n",
      "loss/actor 2.1959304809570312\n",
      "loss/critic 0.1710609793663025\n",
      "loss/actor -1.2209959030151367\n",
      "Episode 1140 finished | Episode reward -124.176791\n",
      "Episode 1140 finished | Average reward -145.589139\n",
      "loss/critic 0.32982853055000305\n",
      "loss/actor 6.968548774719238\n",
      "loss/critic 0.24439039826393127\n",
      "loss/actor -0.42918306589126587\n",
      "loss/critic 0.23138181865215302\n",
      "loss/actor 3.432001829147339\n",
      "loss/critic 0.2833462357521057\n",
      "loss/actor 3.1006100177764893\n",
      "Episode 1160 finished | Episode reward -132.661175\n",
      "Episode 1160 finished | Average reward -131.858020\n",
      "loss/critic 0.17652487754821777\n",
      "loss/actor 0.8369205594062805\n",
      "loss/critic 0.24096207320690155\n",
      "loss/actor 1.6970229148864746\n",
      "loss/critic 0.2629089057445526\n",
      "loss/actor 3.884477376937866\n",
      "loss/critic 0.21711501479148865\n",
      "loss/actor 1.6243352890014648\n",
      "Episode 1180 finished | Episode reward -347.434088\n",
      "Episode 1180 finished | Average reward -248.831708\n",
      "loss/critic 0.13221392035484314\n",
      "loss/actor 1.9795823097229004\n",
      "loss/critic 0.4096204936504364\n",
      "loss/actor 2.9924144744873047\n",
      "loss/critic 0.2928033471107483\n",
      "loss/actor 2.7194929122924805\n",
      "loss/critic 0.2342909574508667\n",
      "loss/actor 3.572173595428467\n",
      "Episode 1200 finished | Episode reward -1104.761418\n",
      "Episode 1200 finished | Average reward -457.053916\n",
      "loss/critic 0.3018379807472229\n",
      "loss/actor 4.009045124053955\n",
      "loss/critic 0.37728604674339294\n",
      "loss/actor 7.054293632507324\n",
      "loss/critic 0.12458626180887222\n",
      "loss/actor 2.5347297191619873\n",
      "loss/critic 0.42912793159484863\n",
      "loss/actor 8.577372550964355\n",
      "Episode 1220 finished | Episode reward -1276.943327\n",
      "Episode 1220 finished | Average reward -764.067130\n",
      "loss/critic 0.36679157614707947\n",
      "loss/actor 5.509433746337891\n",
      "loss/critic 0.2998219132423401\n",
      "loss/actor 4.836925029754639\n",
      "loss/critic 0.48774951696395874\n",
      "loss/actor 7.456872940063477\n",
      "loss/critic 0.5178778171539307\n",
      "loss/actor 8.706515312194824\n",
      "Episode 1240 finished | Episode reward -1232.123086\n",
      "Episode 1240 finished | Average reward -858.042732\n",
      "loss/critic 0.3486044406890869\n",
      "loss/actor 10.710161209106445\n",
      "loss/critic 0.42640596628189087\n",
      "loss/actor 7.4881415367126465\n",
      "loss/critic 0.5138205885887146\n",
      "loss/actor 10.711660385131836\n",
      "loss/critic 0.4591629207134247\n",
      "loss/actor 13.45824909210205\n",
      "Episode 1260 finished | Episode reward -126.261780\n",
      "Episode 1260 finished | Average reward -317.643940\n",
      "loss/critic 0.5687738656997681\n",
      "loss/actor 10.304680824279785\n",
      "loss/critic 0.7800321578979492\n",
      "loss/actor 18.147476196289062\n",
      "loss/critic 0.5792967081069946\n",
      "loss/actor 11.56981372833252\n",
      "loss/critic 0.5677594542503357\n",
      "loss/actor 13.100220680236816\n",
      "Episode 1280 finished | Episode reward -5.491857\n",
      "Episode 1280 finished | Average reward -736.282455\n",
      "loss/critic 0.49837642908096313\n",
      "loss/actor 9.613579750061035\n",
      "loss/critic 0.5789971947669983\n",
      "loss/actor 12.274805068969727\n",
      "loss/critic 0.8768652677536011\n",
      "loss/actor 11.673877716064453\n",
      "loss/critic 0.8551803827285767\n",
      "loss/actor 18.263011932373047\n",
      "Episode 1300 finished | Episode reward -130.559544\n",
      "Episode 1300 finished | Average reward -222.791889\n",
      "loss/critic 0.5854151844978333\n",
      "loss/actor 8.877930641174316\n",
      "loss/critic 1.1500513553619385\n",
      "loss/actor 16.061403274536133\n",
      "loss/critic 0.6225295662879944\n",
      "loss/actor 12.963016510009766\n",
      "loss/critic 0.6115524768829346\n",
      "loss/actor 14.843852043151855\n",
      "Episode 1320 finished | Episode reward -254.150782\n",
      "Episode 1320 finished | Average reward -182.602933\n",
      "loss/critic 0.7334450483322144\n",
      "loss/actor 19.092435836791992\n",
      "loss/critic 0.9033046364784241\n",
      "loss/actor 14.646377563476562\n",
      "loss/critic 0.7642359137535095\n",
      "loss/actor 18.76218605041504\n",
      "loss/critic 0.6788834929466248\n",
      "loss/actor 13.654363632202148\n",
      "Episode 1340 finished | Episode reward -129.631128\n",
      "Episode 1340 finished | Average reward -182.252243\n",
      "loss/critic 0.7700182199478149\n",
      "loss/actor 12.940381050109863\n",
      "loss/critic 1.827985167503357\n",
      "loss/actor 14.133698463439941\n",
      "loss/critic 0.8609672784805298\n",
      "loss/actor 10.866470336914062\n",
      "loss/critic 1.494215488433838\n",
      "loss/actor 14.332247734069824\n",
      "Episode 1360 finished | Episode reward -123.491174\n",
      "Episode 1360 finished | Average reward -157.683660\n",
      "loss/critic 0.8837636709213257\n",
      "loss/actor 19.0721435546875\n",
      "loss/critic 0.7069301605224609\n",
      "loss/actor 9.36939811706543\n",
      "loss/critic 0.5823227167129517\n",
      "loss/actor 8.332649230957031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-54594a186cb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# actorとcriticを更新\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/IMPALA-Pacman/DDPG/DDPG_Agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mreward_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewards'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mterminate_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'terminates'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mnext_q_value_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;31m#target-Q-network内の、対応する行動のインデックスにおける価値関数の値を取ってくる\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mnext_q_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_obs_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_q_value_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Downloads/IMPALA-Pacman/DDPG/DDPG.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_mean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_halfwidth\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_last\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1608\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1610\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1611\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1612\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from DDPG_Agent import DdpgAgent\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import pyglet\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "from collections import deque\n",
    "from GymWrapper import ImgWrapper\n",
    "from util import layer_init\n",
    "from Noise import ActionNoise\n",
    "\n",
    "env = None\n",
    "is_image = False\n",
    "if is_image:\n",
    "    env = ImgWrapper(gym.make('Pendulum-v0'), gray_scale=True)\n",
    "else:\n",
    "    env = gym.make('Pendulum-v0')\n",
    "#検証用にシードを固定する\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "env.seed(42)\n",
    "\n",
    "num_episode = 5000  # 学習エピソード数（学習に時間がかかるので短めにしています）\n",
    "memory_size = 100000  # replay bufferの大きさ\n",
    "initial_memory_size = 10000  # 最初に貯めるランダムな遷移の数\n",
    "# ログ用の設定\n",
    "episode_rewards = []\n",
    "num_average_epidodes = 10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "writer = SummaryWriter(log_dir='./runs')\n",
    "max_steps = env.spec.max_episode_steps  # エピソードの最大ステップ数\n",
    "agent = DdpgAgent(env.observation_space,\n",
    "                  env.action_space,\n",
    "                  device,\n",
    "                  batch_size = 64,\n",
    "                  memory_size=memory_size,\n",
    "                  writer=writer,\n",
    "                  is_image = is_image)\n",
    "\n",
    "# 最初にreplay bufferにランダムな行動をしたときのデータを入れる\n",
    "state = env.reset()\n",
    "for step in range(initial_memory_size):\n",
    "    action = env.action_space.sample() # ランダムに行動を選択 \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    agent.memory.add(state, action, reward, done)\n",
    "    state = env.reset() if done else next_state\n",
    "print('%d Data collected' % (initial_memory_size))\n",
    "\n",
    "for episode in range(num_episode):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    noise = ActionNoise(env.action_space.shape[0])\n",
    "    for t in range(max_steps):\n",
    "        action = agent.get_action(state, noise, t)  #  行動を選択\n",
    "        next_state, reward, done, _ = env.step(action * env.action_space.high)\n",
    "        episode_reward += reward\n",
    "        agent.memory.add(state, action, reward, done)\n",
    "        agent.update()  # actorとcriticを更新\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    episode_rewards.append(episode_reward)\n",
    "    writer.add_scalar(\"reward\", episode_reward, episode)\n",
    "    if episode % 20 == 0:\n",
    "        print(\"Episode %d finished | Episode reward %f\" % (episode, episode_reward))\n",
    "        sum_reward = 0.0\n",
    "        for k in range(50):\n",
    "            state = env.reset()\n",
    "            done = False\n",
    "            step = 0\n",
    "            while not done and step < max_steps:\n",
    "                step += 1\n",
    "                action = agent.get_action(state)  #  行動を選択\n",
    "                next_state, reward, done, _ = env.step(action * env.action_space.high)\n",
    "                sum_reward += reward\n",
    "                state = next_state\n",
    "        print(\"Episode %d finished | Average reward %f\" % (episode, sum_reward / 50))\n",
    "\n",
    "# 累積報酬の移動平均を表示\n",
    "moving_average = np.convolve(episode_rewards, np.ones(num_average_epidodes)/num_average_epidodes, mode='valid')\n",
    "plt.plot(np.arange(len(moving_average)),moving_average)\n",
    "plt.title('DDPG: average rewards in %d episodes' % num_average_epidodes)\n",
    "plt.xlabel('episode')\n",
    "plt.ylabel('rewards')\n",
    "plt.show()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
